# ğŸ  Housing Data Pipeline Service (1975â€“2024)

An automated backend service for ingesting, validating, and serving long-term U.S. housing market data through a stable HTTP API.  
This project focuses on **data reliability, workflow automation, and production awareness**, rather than one-off analysis or dashboards.

---

## ğŸš© Problem Statements

Housing market data is commonly distributed as static files (CSV/Excel) that must be repeatedly imported, cleaned, and recalculated by each consumer. This leads to:

- duplicated data processing  
- inconsistent metric definitions  
- silent data quality issues  
- difficulty tracking data freshness and failures  

As a result, dashboards, scripts, and analysts often disagree on results and lack visibility into data reliability.

---

## âœ… Solution

This project provides a **centralized backend service** that:

- ingests housing data through a repeatable pipeline  
- validates and enforces data quality rules  
- quarantines invalid data instead of silently failing  
- records ingestion run history for auditability  
- triggers alerts on failure or partial success  
- exposes trusted metrics and time-series data via HTTP APIs  

The system is designed to be **operated**, not just run once.

---

## ğŸ§  Key Design Concepts

- **Outcome-based workflows**  
  Ingestion runs explicitly result in `SUCCESS`, `PARTIAL`, or `FAILURE`, each triggering different actions.

- **Single source of truth**  
  All consumers rely on the same validated data and metric definitions.

- **Separation of concerns**  
  Ingestion, validation, storage, and API access are clearly separated.

- **Reproducible environment**  
  The entire system runs via Docker for consistency across machines.

---

## ğŸ—ï¸ Architecture Overview

**Data flow:**

1. Raw housing data is ingested into a staging layer  
2. Validation rules are applied  
3. Valid rows are promoted to clean storage  
4. Invalid rows are quarantined with error reasons  
5. Ingestion results are recorded and alerts generated  
6. Clients consume data through HTTP APIs  

---

## ğŸ—„ï¸ Database Structure

Core tables include:

- `housing_raw` â€“ staging area for incoming data  
- `housing_clean` â€“ validated, trusted housing data  
- `quarantine_rows` â€“ rejected rows with validation errors  
- `ingestion_runs` â€“ audit trail of ingestion outcomes  
- `alerts` â€“ warnings and failures triggered by workflows  

Uniqueness and integrity are enforced at the database level.

---

## ğŸ”Œ API Endpoints

### Operational
- `GET /health` â€“ service health check  
- `GET /meta/last-run` â€“ most recent ingestion status  
- `GET /runs` â€“ ingestion run history  
- `GET /runs/{id}` â€“ detailed run information  
- `GET /alerts` â€“ recent alerts  

### Data Access
- `GET /states` â€“ available states  
- `GET /series/{state}` â€“ quarterly time-series data  
- `GET /metrics/{state}` â€“ computed housing metrics  
- `GET /rankings?metric=total_growth&top=10` â€“ ranked results  

All endpoints return JSON and are documented via OpenAPI.

---

## âš™ï¸ Running the Project

### Prerequisites
- Docker  
- Docker Compose  

### Start the system
```bash
docker compose up --build


